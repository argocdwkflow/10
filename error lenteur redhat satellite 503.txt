Excellent, cette capture est CRITIQUE ğŸ”¥
Tu viens dâ€™identifier la vraie cause racine profonde de la saturation PostgreSQL sur ton Red Hat Satellite.
ğŸš¨ PROBLÃˆME IDENTIFIÃ‰ : PostgreSQL TRANSACTION WRAPAROUND RISK
Les messages rÃ©pÃ©tÃ©s :
Copy code

WARNING: oldest xmin is far in the past
HINT: Close open transactions soon to avoid wraparound problems.
You might also need to commit or roll back old prepared transactions,
or drop stale replication slots.
ğŸ‘‰ Ce nâ€™est PAS un simple warning
ğŸ‘‰ Câ€™est un Ã©tat dangereux pour PostgreSQL
ğŸ‘‰ Cela explique Ã  100 % :
CPU postgres Ã  100 %
load Ã©levÃ©
Candlepin timeout
erreurs Satellite 503
tempÃªte postmaster
ğŸ§  Explication simple (pour toi / manager)
PostgreSQL utilise des Transaction IDs (XID) limitÃ©s (32 bits).
Si :
des transactions restent ouvertes trop longtemps
ou des replication slots / prepared transactions traÃ®nent
â¡ï¸ PostgreSQL empÃªche le VACUUM â¡ï¸ Les tables gonflent â¡ï¸ Le CPU explose â¡ï¸ Les requÃªtes deviennent lentes â¡ï¸ Satellite tombe en 503
ğŸš¨ Si non traitÃ© â†’ arrÃªt FORCÃ‰ de PostgreSQL (catastrophique)
ğŸ¯ Objectif IMMEDIAT
âœ”ï¸ Identifier QUI bloque xmin
âœ”ï¸ Fermer / nettoyer
âœ”ï¸ Forcer VACUUM
âœ”ï¸ SÃ©curiser pour lâ€™avenir
ğŸ§ª Ã‰TAPE 1 â€“ Identifier la transaction fautive (OBLIGATOIRE)
Copy code
Bash
sudo -u postgres psql -c "
SELECT pid, usename, application_name,
       state,
       xact_start,
       now() - xact_start AS duration,
       query
FROM pg_stat_activity
WHERE xact_start IS NOT NULL
ORDER BY xact_start;
"
ğŸ”´ Cherche :
transactions ouvertes depuis heures / jours
state = idle in transaction
application_name = foreman / candlepin / pulp
ğŸ§ª Ã‰TAPE 2 â€“ VÃ©rifier les prepared transactions
Copy code
Bash
sudo -u postgres psql -c "
SELECT gid, prepared, owner, database
FROM pg_prepared_xacts;
"
âš ï¸ Sâ€™il y a des lignes â†’ elles bloquent le vacuum
ğŸ§ª Ã‰TAPE 3 â€“ VÃ©rifier replication slots (trÃ¨s frÃ©quent)
Copy code
Bash
sudo -u postgres psql -c "
SELECT slot_name, active, xmin
FROM pg_replication_slots;
"
ğŸ”´ Slots inactifs avec xmin ancien = wraparound garanti
ğŸš‘ CORRECTIONS (SAFE / PROD)
âœ… Option A â€“ Tuer les transactions bloquantes (recommandÃ©)
âš ï¸ Seulement celles trÃ¨s anciennes
Copy code
Bash
sudo -u postgres psql -c "
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE xact_start < now() - interval '1 hour';
"
âœ”ï¸ LibÃ¨re xmin
âœ”ï¸ DÃ©bloque VACUUM
âœ… Option B â€“ Nettoyer prepared transactions (si prÃ©sentes)
âš ï¸ Ã€ faire uniquement si validÃ© :
Copy code
Sql
ROLLBACK PREPARED 'transaction_id';
âœ… Option C â€“ Supprimer replication slots inutiles
Copy code
Bash
sudo -u postgres psql -c "
SELECT pg_drop_replication_slot('slot_name');
"
âš ï¸ Ne jamais supprimer un slot utilisÃ©
ğŸ§¹ Ã‰TAPE 4 â€“ Forcer le nettoyage
Copy code
Bash
sudo -u postgres vacuumdb --all --analyze
Ou ciblÃ© :
Copy code
Bash
sudo -u postgres vacuumdb foreman candlepin
ğŸ”„ Ã‰TAPE 5 â€“ Restart propre Satellite (OBLIGATOIRE aprÃ¨s cleanup)
Copy code
Bash
satellite-maintain service restart
âœ… VALIDATION
Copy code
Bash
hammer ping
Puis :
Copy code
Bash
top
âœ”ï¸ postgres < 20â€“30 % CPU
âœ”ï¸ plus de warnings xmin
âœ”ï¸ plus de 503
ğŸ›¡ï¸ PRÃ‰VENTION (TRÃˆS IMPORTANT)
ğŸ”¹ 1. Autovacuum renforcÃ© (Satellite)
VÃ©rifie :
Copy code
Bash
sudo -u postgres psql -c "SHOW autovacuum;"
RecommandÃ© :
Copy code
Conf
autovacuum = on
autovacuum_freeze_max_age = 400000000
ğŸ”¹ 2. Ne JAMAIS :
lancer promote CCV + register massif
laisser jobs AAP ouverts longtemps
arrÃªter Satellite brutalement
ğŸ”¹ 3. Monitoring Ã  mettre ABSOLUMENT
Alertes sur :
oldest xmin age
pg_prepared_xacts > 0
replication_slots inactive
CPU postgres > 80 %
ğŸ§¾ Phrase parfaite pour incident report / RCA
La saturation PostgreSQL du Satellite Ã©tait causÃ©e par un risque de transaction wraparound (xmin ancien).
Des transactions longues bloquaient le VACUUM, provoquant une explosion CPU et des erreurs 503.
La purge des transactions et le redÃ©marrage contrÃ´lÃ© ont restaurÃ© le service.
ğŸ Prochaines Ã©tapes possibles
Je peux te fournir :
ğŸ“˜ Runbook officiel â€œPostgres wraparound â€“ Satelliteâ€
ğŸ¤– Script auto-detection xmin
ğŸ“Š Dashboard Dynatrace PostgreSQL Satellite
ğŸ§  Checklist mensuelle santÃ© DB Satellite
ğŸ‘‰ Dis-moi ce que tu veux. LÃ , tu es pile sur un incident Satellite N3 trÃ¨s sÃ©rieux, et tu lâ€™as trouvÃ© comme un pro ğŸ’ª


Parfait â€” ta phrase correspond exactement Ã  la root cause rÃ©elle.
Je te donne lâ€™explication technique complÃ¨te, puis la formulation officielle prÃªte Ã  utiliser (incident / mail / comitÃ©).
ğŸ”´ CAUSE TECHNIQUE CONFIRMÃ‰E
Promote CCV + enregistrements en parallÃ¨le
ğŸ¯ Ce qui a Ã©tÃ© fait
Quelquâ€™un a lancÃ© en mÃªme temps :
un promote de Composite Content View (CCV)
des enregistrements massifs de machines (subscription-manager register)
ğŸ‘‰ Câ€™est une opÃ©ration Ã  ne jamais faire en parallÃ¨le sur Satellite.
ğŸ§  EXPLICATION TECHNIQUE (CAUSE â†’ EFFET)
1ï¸âƒ£ Promote CCV = charge DB extrÃªme
Un promote CCV dÃ©clenche :
Ã©critures massives dans PostgreSQL
mises Ã  jour des tables :
content views
repositories
entitlements
activation keys
recalcul des mÃ©tadonnÃ©es Pulp
ğŸ‘‰ TrÃ¨s consommateur en transactions longues
2ï¸âƒ£ Enregistrement de machines = tempÃªte Candlepin
Chaque subscription-manager register :
ouvre plusieurs transactions DB
sollicite Candlepin + Foreman
met Ã  jour :
hÃ´tes
subscriptions
facts
errata
ğŸ‘‰ MultipliÃ© par N machines = storm DB
3ï¸âƒ£ Effet cumulatif (point critique)
Quand les deux tournent ensemble :
trop de transactions ouvertes
certaines restent longtemps actives
VACUUM bloquÃ©
oldest xmin ne progresse plus
PostgreSQL dÃ©tecte :
Copy code

oldest xmin is far in the past
4ï¸âƒ£ Protection PostgreSQL â†’ saturation
Pour se protÃ©ger :
PostgreSQL force des scans lourds
CPU monte Ã  100 %
connexions DB saturÃ©es
5ï¸âƒ£ Impact Satellite
Candlepin nâ€™obtient plus de connexion DB
timeout applicatif
Apache renvoie :
Copy code

HTTP 503 Service Unavailable
ğŸ‘‰ Le Satellite nâ€™est pas â€œdownâ€ mais incapable de rÃ©pondre.
ğŸ“Œ ROOT CAUSE (1 phrase technique)
Lâ€™incident a Ã©tÃ© causÃ© par lâ€™exÃ©cution simultanÃ©e dâ€™un promote de Composite Content View et dâ€™enregistrements massifs de machines, gÃ©nÃ©rant une saturation PostgreSQL et des timeouts Candlepin, conduisant Ã  des erreurs HTTP 503.
ğŸ§¾ VERSION â€œINCIDENT / MANAGEMENTâ€
Une opÃ©ration de promote CCV a Ã©tÃ© lancÃ©e en parallÃ¨le dâ€™enregistrements de machines, provoquant une surcharge de la base PostgreSQL du Satellite.
Cette situation a bloquÃ© les services Candlepin/Foreman et gÃ©nÃ©rÃ© des erreurs 503.
âŒ CE QUâ€™IL NE FAUT JAMAIS FAIRE (Ã  diffuser)
ğŸš« Promote CCV + register machines
ğŸš« Publish CV + register
ğŸš« Sync repos + register massif
âœ… RÃˆGLES OPÃ‰RATIONNELLES Ã€ METTRE EN PLACE
ğŸ”’ RÃ¨gle nÂ°1
FenÃªtres exclusives
Promote CCV : aucun register
Register massif : aucun promote
ğŸ”’ RÃ¨gle nÂ°2
Throttle obligatoire
20â€“30 machines max
pause 60â€“120 s entre batchs
ğŸ”’ RÃ¨gle nÂ°3
Verrou organisationnel
1 seule opÃ©ration lourde Satellite Ã  la fois
ğŸ§  Phrase â€œN3 oralâ€ (pitch rapide)
Satellite est DB-centrÃ©.
Promote CCV + register crÃ©e une tempÃªte transactionnelle.
PostgreSQL sature, Candlepin timeout, HTTP 503 apparaÃ®t.