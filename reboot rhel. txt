Voici une analyse complÃ¨te et structurÃ©e des causes racines (Root Cause Analysis â€“ RCA) dâ€™un reboot dâ€™un serveur Linux RHEL 7, comme on lâ€™attend en environnement production / N3.
ğŸ” MÃ©thodologie RCA recommandÃ©e (RHEL 7)
1ï¸âƒ£ Identifier si le reboot est volontaire ou subi
Câ€™est lâ€™Ã©tape la plus critique.
Type
Indication
ğŸ”„ Reboot volontaire
reboot, shutdown, patching
âš ï¸ Crash OS
kernel panic, OOM, soft lockup
ğŸ”Œ Reboot matÃ©riel
power loss, iDRAC, watchdog
ğŸ” Action sÃ©curitÃ©
hardening, agent EDR
2ï¸âƒ£ Sources de logs clÃ©s Ã  analyser (dans lâ€™ordre)
ğŸ”¹ Historique des reboots
Copy code
Bash
last -x | head -20
RÃ©sultats typiques :
reboot system boot
crash
shutdown
ğŸ”¹ Logs systÃ¨me
Copy code
Bash
grep -iE "reboot|panic|oom|watchdog|reset" /var/log/messages
Copy code
Bash
journalctl --list-boots
journalctl -b -1
3ï¸âƒ£ Causes racines les plus frÃ©quentes sur RHEL 7
ğŸŸ¥ A. Kernel Panic (cause critique)
SymptÃ´mes :
reboot brutal
message kernel panic ou Oops
Causes :
bug kernel
module tiers (Veritas, Oracle ASM, drivers SAN)
corruption mÃ©moire
VÃ©rification :
Copy code
Bash
journalctl -k -b -1
ğŸ“Œ RCA type :
Kernel panic causÃ© par le module vxfs aprÃ¨s une opÃ©ration I/O concurrente.
ğŸŸ¥ B. OOM Killer (Out Of Memory)
SymptÃ´mes :
charge Ã©levÃ©e
base/app stoppÃ©e
parfois reboot indirect
Logs :
Copy code
Bash
grep -i oom /var/log/messages
Cause racine :
RAM saturÃ©e
swap insuffisant
memory leak applicatif
ğŸŸ¥ C. Watchdog matÃ©riel / logiciel
TrÃ¨s frÃ©quent sur serveurs Dell / HP / IBM.
Logs :
Copy code
Bash
grep -i watchdog /var/log/messages
Causes :
CPU soft lockup
kernel bloquÃ© > timeout watchdog
ğŸ“Œ Exemple RCA :
Reboot dÃ©clenchÃ© par watchdog suite Ã  un soft lockup CPU causÃ© par une contention I/O.
ğŸŸ¥ D. ProblÃ¨me matÃ©riel / hyperviseur
Indications :
Aucun log OS avant reboot
reboot â€œpropreâ€ mais inexpliquÃ©
Causes :
panne alimentation
crash hyperviseur (VMware / IBM Cloud)
reset via iDRAC / IMM
ğŸ‘‰ Ã€ vÃ©rifier :
logs hyperviseur
logs iDRAC / iLO
ğŸŸ¥ E. Action humaine / automatisation
TrÃ¨s courant en environnement Satellite / Ansible.
Indices :
Copy code
Bash
last reboot
ausearch -x reboot
Causes possibles :
patching automatique
cron mal configurÃ©
job Ansible / Satellite
ğŸŸ¥ F. SÃ©curitÃ© / agents
Agents type :
antivirus
EDR
monitoring avec injection kernel
SymptÃ´me :
reboot post-update agent
kernel tainted
Copy code
Bash
dmesg | grep -i taint
4ï¸âƒ£ Cas spÃ©cifique RHEL 7 (Ã  connaÃ®tre)
Ã‰lÃ©ment
Risque
Kernel ancien
bugs connus
Modules tiers
instabilitÃ©
cgroups v1
OOM plus brutal
systemd ancien
logs incomplets
5ï¸âƒ£ ModÃ¨le de Conclusion RCA professionnelle
Root Cause : Reboot non planifiÃ© du serveur causÃ© par un kernel panic dÃ©clenchÃ© par un module tiers Veritas lors dâ€™une opÃ©ration I/O intensive.
Impact : Interruption applicative de 4 minutes.
Corrective Action : Mise Ã  jour du module, ajustement watchdog, upgrade kernel validÃ©.
Preventive Action : Monitoring kernel panic, validation modules tiers aprÃ¨s patching.
6ï¸âƒ£ Si tu veux, je peux :
âœ… Tâ€™aider Ã  analyser un extrait rÃ©el de /var/log/messages
âœ… RÃ©diger une RCA prÃªte pour incident manager
âœ… Comparer RHEL7 vs RHEL8/9 sur les causes de reboot
âœ… Fournir un script dâ€™audit reboot automatique
Dis-moi ğŸ‘

Voici une liste avancÃ©e de commandes de troubleshooting Ã  lancer aprÃ¨s un reboot RHEL 7, utilisÃ©es en diagnostic N3 / RCA production.
Je les ai classÃ©es par objectif, avec interprÃ©tation attendue.
ğŸ§  1ï¸âƒ£ Comprendre POURQUOI le kernel sâ€™est arrÃªtÃ©
ğŸ”¹ Derniers messages kernel AVANT reboot
Copy code
Bash
journalctl -k -b -1 --no-pager | tail -200
â¡ï¸ Cherche :
kernel panic
BUG: soft lockup
watchdog
oom-killer
ğŸ”¹ Kernel buffer brut
Copy code
Bash
dmesg -T | tail -200
ğŸ”¹ Ã‰tat kernel (tainted = module tiers)
Copy code
Bash
cat /proc/sys/kernel/tainted
0 â†’ kernel sain
>0 â†’ module propriÃ©taire / instabilitÃ© possible
ğŸ§® 2ï¸âƒ£ MÃ©moire / OOM / Pression RAM
ğŸ”¹ Historique OOM
Copy code
Bash
grep -i "oom" /var/log/messages
ğŸ”¹ Dernier Ã©tat mÃ©moire
Copy code
Bash
free -m
vmstat 1 5
ğŸ”¹ ParamÃ¨tres mÃ©moire critiques
Copy code
Bash
sysctl vm.overcommit_memory
sysctl vm.swappiness
ğŸ“Œ Indice RCA : OOM + swap faible â†’ reboot indirect par watchdog ou crash applicatif critique.
âš™ï¸ 3ï¸âƒ£ CPU / soft lockup / watchdog
ğŸ”¹ Soft lockup CPU
Copy code
Bash
grep -i "soft lockup" /var/log/messages
ğŸ”¹ Watchdog
Copy code
Bash
grep -i watchdog /var/log/messages
ğŸ”¹ Charge historique
Copy code
Bash
uptime
sar -q 1 5
ğŸ’¾ 4ï¸âƒ£ Disques / I/O / Filesystems
ğŸ”¹ Erreurs filesystem
Copy code
Bash
grep -iE "xfs|ext4|I/O error" /var/log/messages
ğŸ”¹ Latence disque
Copy code
Bash
iostat -xz 1 5
ğŸ”¹ Filesystem en erreur (forÃ§ant reboot)
Copy code
Bash
mount | grep ro
ğŸ”Œ 5ï¸âƒ£ RÃ©seau / drivers / bonding
ğŸ”¹ Erreurs drivers
Copy code
Bash
dmesg | grep -iE "eth|bond|net|link down"
ğŸ”¹ Ã‰tat bonding
Copy code
Bash
cat /proc/net/bonding/bond0
ğŸ“Œ Cas rÃ©el : driver NIC crash â†’ kernel panic.
ğŸ§© 6ï¸âƒ£ Modules kernel tiers (TRÃˆS IMPORTANT)
ğŸ”¹ Modules chargÃ©s
Copy code
Bash
lsmod | head -20
ğŸ”¹ Modules suspects
Copy code
Bash
lsmod | egrep "vx|oracle|asm|kmod|falcon|edr"
ğŸ”¹ Version kernel
Copy code
Bash
uname -a
rpm -q kernel
ğŸ“Œ RCA frÃ©quent RHEL7 : kernel patchÃ© + module non compatible.
ğŸ” 7ï¸âƒ£ SÃ©curitÃ© / actions humaines / automation
ğŸ”¹ Qui a rebootÃ© ?
Copy code
Bash
last reboot
last shutdown
ğŸ”¹ Audit
Copy code
Bash
ausearch -i -m system_boot,system_shutdown
ğŸ”¹ Cron / automatisation
Copy code
Bash
crontab -l
ls /etc/cron.*
ğŸ–¥ï¸ 8ï¸âƒ£ MatÃ©riel / hyperviseur (si VM)
ğŸ”¹ Indice power loss
Copy code
Bash
journalctl -b -1 | grep -i power
â¡ï¸ Si aucun log OS avant reboot â†’
ğŸ‘‰ cause externe (VMware, IBM Cloud, iDRAC)
ğŸ“¦ 9ï¸âƒ£ Services critiques avant reboot
Copy code
Bash
systemctl --failed
systemctl list-units --state=failed
ğŸ“Š 10ï¸âƒ£ Diagnostic post-mortem rapide (script N3)
Copy code
Bash
echo "=== LAST BOOTS ==="; last -x | head
echo "=== KERNEL ERRORS ==="; journalctl -k -b -1 | tail -50
echo "=== OOM ==="; grep -i oom /var/log/messages
echo "=== WATCHDOG ==="; grep -i watchdog /var/log/messages
echo "=== TAINT ==="; cat /proc/sys/kernel/tainted
ğŸ§¾ Conclusion N3 (exemple)
Root Cause : Reboot non planifiÃ© causÃ© par un soft lockup CPU dÃ©tectÃ© par le watchdog matÃ©riel, dÃ©clenchÃ© par une saturation I/O disque combinÃ©e Ã  un module tiers non certifiÃ© sur kernel RHEL 7.
Corrective Action : Upgrade kernel + module, ajustement watchdog.
Preventive : Monitoring I/O + validation modules aprÃ¨s patch.
ğŸ‘‰ Si tu veux, envoie-moi :
un /var/log/messages
un journalctl -b -1
ou dis-moi VM ou physique
Je te ferai la RCA finale prÃªte incident manager / client âœ…

OK â€” avec uniquement la date/heure 06 janvier 2026 Ã  09:37 je ne peux pas â€œdevinerâ€ la root cause (il faut une preuve dans les logs). Par contre je peux te donner la procÃ©dure exacte pour lâ€™identifier Ã  100% et sortir une RCA propre.
HypothÃ¨se : serveur en Europe/Paris (CET). Si lâ€™horodatage est en UTC sur certains logs, on le verra.
1) Confirmer lâ€™Ã©vÃ©nement reboot autour de 09:37
Copy code
Bash
last -x | egrep -i "reboot|shutdown" | head -30
who -b
uptime
Ce que tu cherques :
une ligne reboot   system boot  ... proche de Tue Jan  6 09:37
ou shutdown juste avant.
2) Extraire les logs exactement autour de 09:37 (fenÃªtre 10 min)
Si systemd/journald dispo (RHEL7 oui)
Copy code
Bash
journalctl --since "2026-01-06 09:25:00" --until "2026-01-06 09:50:00" --no-pager
Kernel uniquement (souvent la root cause est lÃ )
Copy code
Bash
journalctl -k --since "2026-01-06 09:25:00" --until "2026-01-06 09:50:00" --no-pager
Logs classiques RHEL7
Copy code
Bash
sed -n '/Jan  6 09:2[5-9]/,/Jan  6 09:5[0-9]/p' /var/log/messages
3) Chercher les â€œsignaturesâ€ root cause (patterns)
Lance ces commandes sur la mÃªme fenÃªtre horaire :
A) Kernel panic / oops / lockup
Copy code
Bash
egrep -i "panic|Oops|BUG:|soft lockup|hard lockup|NMI|Call Trace" /var/log/messages
B) OOM killer (mÃ©moire)
Copy code
Bash
egrep -i "Out of memory|oom-killer|Killed process" /var/log/messages
C) Watchdog (matÃ©riel / softdog)
Copy code
Bash
egrep -i "watchdog|softdog|hardlockup" /var/log/messages
D) Storage / filesystem (I/O error => panic parfois)
Copy code
Bash
egrep -i "I/O error|blk_update_request|Buffer I/O|xfs|ext4|reset|nvme|sd .* error" /var/log/messages
E) Power / ACPI / reset externe (souvent â€œaucun logâ€ avant reboot)
Copy code
Bash
egrep -i "Power|ACPI|reset|Machine check|mce|thermal" /var/log/messages
F) Reboot demandÃ© par quelquâ€™un / automation
Copy code
Bash
egrep -i "shutdown|reboot|systemd-logind|Runlevel|init 6" /var/log/messages
4) VÃ©rifier si câ€™est un CRASH kernel (kdump)
Si kdump est actif, tu peux avoir la preuve dÃ©finitive :
Copy code
Bash
systemctl status kdump
ls -ltr /var/crash/
Sâ€™il existe un rÃ©pertoire datÃ© du 2026-01-06, câ€™est trÃ¨s probablement kernel panic / crash.
5) Indices perf juste avant 09:37 (si sysstat installÃ©)
Copy code
Bash
sar -q -f /var/log/sa/sa06 | sed -n '1,200p'
sar -u -f /var/log/sa/sa06 | sed -n '1,200p'
sar -r -f /var/log/sa/sa06 | sed -n '1,200p'
sar -d -f /var/log/sa/sa06 | sed -n '1,200p'
ğŸ‘‰ Cherche un pic Ã  09:30â€“09:37 :
load Ã©norme (CPU lockup)
mÃ©moire qui tombe Ã  0 (OOM)
iowait trÃ¨s haut (storage freeze â†’ watchdog)
6) Comment conclure la root cause (rÃ¨gle simple)
Tu as un â€œpanic/oops/call traceâ€ â†’ root cause = kernel crash (souvent module/driver).
Tu as â€œOut of memory / Killed processâ€ â†’ root cause = OOM / fuite mÃ©moire / sizing.
Tu as â€œwatchdog: hard/soft lockupâ€ â†’ root cause = CPU hang (souvent I/O ou bug kernel).
Tu nâ€™as rien dans les logs OS juste avant â†’ root cause = externe (hyperviseur, iDRAC, perte dâ€™alim).
Ce dont jâ€™ai besoin pour te donner la root cause exacte
Colle ici (mÃªme tronquÃ©) :
sortie de :
Copy code
Bash
journalctl -k --since "2026-01-06 09:25" --until "2026-01-06 09:50" --no-pager | tail -200
et :
Copy code
Bash
sed -n '/Jan  6 09:3[0-9]/,/Jan  6 09:4[0-9]/p' /var/log/messages
Avec Ã§a, je te rÃ©dige la RCA finale (Root Cause + Evidence + Corrective/Preventive actions) sur lâ€™incident de 06/01/2026 09:37.





Voici la traduction complÃ¨te et professionnelle en franÃ§ais, prÃªte Ã  Ãªtre intÃ©grÃ©e telle quelle dans un rapport dâ€™incident, une RCA ITIL ou une communication client / management.
âœ… RCA FINALE â€“ Absence de logs entre 04:15 et 09:38
Serveur physique Dell â€“ RHEL 7
ğŸ”´ Conclusion dÃ©finitive
Le serveur nâ€™a pas Ã©tÃ© Ã©teint, nâ€™a pas redÃ©marrÃ©, et nâ€™a pas subi de crash entre 04:15 et 09:38.
Durant cette pÃ©riode, le serveur sâ€™est retrouvÃ© dans un Ã©tat de gel partiel du systÃ¨me dâ€™exploitation (OS hang), trÃ¨s probablement liÃ© Ã  un blocage du sous-systÃ¨me de stockage / I/O.
La reprise observÃ©e vers 09:37â€“09:39 correspond Ã  un reset matÃ©riel (power-on / device reset) dÃ©clenchÃ© au niveau du matÃ©riel Dell, permettant au serveur de reprendre le contrÃ´le et de redÃ©marrer correctement.
ğŸ§¾ Preuves techniques
1ï¸âƒ£ Absence totale de logs systÃ¨me
Aucun log rsyslog
Aucun log journald
Aucun log kernel
Aucune trace de rotation ou dâ€™arrÃªt propre
ğŸ‘‰ Cela indique que le kernel ne parvenait plus Ã  Ã©crire sur le disque, tout en restant alimentÃ©.
â¡ï¸ Ce comportement est caractÃ©ristique dâ€™un gel OS, et non dâ€™un arrÃªt ou dâ€™un crash.
2ï¸âƒ£ Confirmation par la supervision (Dynatrace)
DerniÃ¨res mÃ©triques valides vers 04:10â€“04:15
Aucune donnÃ©e remontÃ©e jusquâ€™Ã  ~09:30
Reprise brutale aprÃ¨s le redÃ©marrage
ğŸ‘‰ Lorsque lâ€™agent de supervision cesse totalement de remonter des donnÃ©es, cela indique que le noyau du systÃ¨me ne rÃ©pond plus, et non un problÃ¨me de lâ€™agent lui-mÃªme.
3ï¸âƒ£ Gel des entrÃ©es/sorties disque
IOPS quasi nuls
DÃ©bit disque â‰ˆ 0
ArrÃªt brutal des mÃ©triques, sans montÃ©e progressive
ğŸ‘‰ Signature typique dâ€™un blocage du chemin de stockage (HBA / SAN / firmware).
4ï¸âƒ£ Preuve matÃ©rielle explicite aprÃ¨s reprise
Dans les logs kernel, on observe des messages rÃ©pÃ©tÃ©s :
Copy code

Power-on or device reset occurred
ğŸ‘‰ Ces messages confirment quâ€™un reset matÃ©riel a eu lieu (rÃ©initialisation des pÃ©riphÃ©riques et du bus), ce qui nâ€™est possible quâ€™aprÃ¨s une intervention matÃ©rielle ou un redÃ©marrage forcÃ©.
5ï¸âƒ£ Erreurs ACPI / IPMI
Copy code

ACPI Error: No handler for Region
IPMI probing via ACPI
ğŸ‘‰ Ces messages sont typiques dâ€™une rÃ©initialisation du contrÃ´leur matÃ©riel (iDRAC/BMC) et confirment une interaction matÃ©rielle aprÃ¨s le gel du systÃ¨me.
â±ï¸ Chronologie reconstituÃ©e
Heure
Ã‰vÃ©nement
~04:10â€“04:15
DerniÃ¨res activitÃ©s normales du systÃ¨me
04:15
DÃ©but du gel OS / blocage I/O
04:15 â€“ 09:37
Serveur alimentÃ© mais non rÃ©actif
09:37
Reset matÃ©riel / power-on dÃ©tectÃ©
09:38â€“09:39
RedÃ©marrage normal du systÃ¨me
AprÃ¨s
Fonctionnement nominal
ğŸ¯ Cause racine (Root Cause)
Le serveur a subi un gel partiel du systÃ¨me dâ€™exploitation, causÃ© par un blocage du sous-systÃ¨me de stockage / entrÃ©es-sorties, empÃªchant lâ€™Ã©criture des logs et la remontÃ©e des mÃ©triques.
Le serveur est restÃ© sous tension mais non rÃ©actif jusquâ€™Ã  ce quâ€™un reset matÃ©riel soit dÃ©clenchÃ©, permettant un redÃ©marrage propre du systÃ¨me.
âŒ Ce qui est exclu
âŒ ProblÃ¨me applicatif
âŒ Saturation mÃ©moire (OOM)
âŒ Kernel panic
âŒ Action utilisateur Linux
âŒ Dysfonctionnement de lâ€™outil de supervision
ğŸ”§ Actions recommandÃ©es
CÃ´tÃ© matÃ©riel (Dell)
Analyse des logs iDRAC
VÃ©rification des Ã©vÃ©nements power/reset
ContrÃ´le des firmwares (BIOS, iDRAC, contrÃ´leur RAID, HBA)
CÃ´tÃ© stockage
Analyse SAN / chemins dâ€™accÃ¨s
VÃ©rification multipath
Timeouts et latences I/O
PrÃ©ventif OS
Activation du hung task watchdog
Activation et validation kdump
Supervision avancÃ©e de la latence disque
Mise Ã  jour du kernel si applicable
ğŸ§¾ Formulation prÃªte client / management
Entre 04:15 et 09:37, le serveur nâ€™a plus produit de logs ni de mÃ©triques tout en restant sous tension. Ce comportement est caractÃ©ristique dâ€™un gel partiel du systÃ¨me dâ€™exploitation, trÃ¨s probablement liÃ© Ã  un blocage du sous-systÃ¨me de stockage.
Ã€ 09:37, un reset matÃ©riel a Ã©tÃ© dÃ©tectÃ©, permettant au serveur de redÃ©marrer normalement. Aucun crash logiciel ni incident applicatif nâ€™a Ã©tÃ© identifiÃ©.
Si tu veux, je peux aussi :
Adapter le texte Ã  un rapport ITIL officiel
Le raccourcir pour un mail management
PrÃ©parer une escalade technique Dell / Storage
Dis-moi ğŸ‘




Oui â€” au vu de tes preuves (trou noir 04:15â†’09:37 + reprise avec beaucoup de Power-on or device reset occurred sur sd/ch/st), un problÃ¨me Storage/SAN/multipathing est lâ€™hypothÃ¨se nÂ°1.
Mais pour Ãªtre carrÃ© en RCA, il faut le prouver avec 2â€“3 indicateurs : perte de chemins, timeouts SCSI, queueing, hung tasks.
Ce que Ã§a ressemble en pratique
Quand un SAN ou un chemin FC/iSCSI â€œfreezeâ€ :
le kernel attend des I/O (bloquÃ©es),
journald/rsyslog nâ€™arrivent plus Ã  Ã©crire â‡’ plus de logs,
lâ€™OS devient â€œgelÃ©â€ (SSH peut ne plus rÃ©pondre),
puis quelquâ€™un fait un reset/power-cycle (Dell/iDRAC) â‡’ tu vois les resets devices au reboot.
Ã€ chercher en prioritÃ© dans les logs (signatures SAN/multipath)
1) Timeouts / erreurs SCSI
Copy code
Bash
grep -iE "scsi|sd .*:.*(timeout|timed out)|blk_update_request|Buffer I/O|I/O error|abort|reset|task abort|hostbyte|driverbyte" /var/log/messages
2) â€œHANGâ€ kernel (process bloquÃ©s en I/O)
Copy code
Bash
grep -iE "blocked for more than|hung_task|INFO: task" /var/log/messages
3) Multipath / device-mapper
Copy code
Bash
grep -iE "multipath|device-mapper|dm-[0-9]+|path down|checker|queue_if_no_path|failover" /var/log/messages
Si tu retrouves ces messages juste avant ~04:15, tu tiens la cause.
VÃ©rifs techniques Ã  faire maintenant (post-incident)
A) Ã‰tat multipath actuel
Copy code
Bash
multipath -ll
multipathd show maps
multipathd show paths
Cherche :
paths en failed/faulty/ghost
â€œqueueingâ€
beaucoup de bascules (failover)
B) Statut FC (si Fibre Channel)
Copy code
Bash
systool -c fc_host -v | egrep -i "ClassDevice|port_state|speed|port_name|node_name"
dmesg -T | egrep -i "lpfc|qla2xxx|fc_host|Link down|Link up"
C) iSCSI (si iSCSI)
Copy code
Bash
iscsiadm -m session -P 3 | egrep -i "Target:|Current Portal:|State:|IFace Name:|Persistent Portal:"
D) Timeout disque / DM
Copy code
Bash
lsscsi -t
dmsetup ls --tree
Indices â€œfortsâ€ qui confirment multipath/SAN
Tu peux conclure â€œStorage/MultiPathâ€ si tu as au moins 2 de ces 4 points :
Logs sdX: timing out command / blk_update_request I/O error
Logs INFO: task ... blocked for more than ... / hung_task
Logs multipath path down, failing path, queue_if_no_path
CÃ´tÃ© infra/storage : alerte SAN, port FC flap, zoning, reboot baie, maintenance
Important : pourquoi tu peux avoir â€œzÃ©ro logâ€ pendant le hang
MÃªme si le kernel dÃ©tecte le souci, il peut ne plus rÃ©ussir Ã  flusher les logs sur disque si /var est sur le SAN/DM multipath, ou si le filesystem est bloquÃ© par des I/O en attente. Donc lâ€™absence de log renforce la piste storage.
Prochaine Ã©tape (la plus efficace)
Envoie-moi :
le rÃ©sultat de :
Copy code
Bash
multipath -ll
multipathd show paths
les 50 lignes autour de 04:15 (si elles existent) :
Copy code
Bash
sed -n '/Jan  6 04:1[0-9]/,/Jan  6 04:2[0-9]/p' /var/log/messages
Et je te dis exactement si câ€™est :
perte de chemins multipath,
HBA/driver FC (lpfc/qla2xxx),
SAN backend,
ou un autre blocage I/O (filesystem/RAID).